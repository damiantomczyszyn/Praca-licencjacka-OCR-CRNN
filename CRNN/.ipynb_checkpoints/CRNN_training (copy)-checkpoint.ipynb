{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xq-IHfAQ15A7",
    "outputId": "0aa3ffed-b203-4ee3-c8c4-d7f491ddf8f0"
   },
   "source": [
    "!mkdir content/\n",
    "%cd content\n",
    "!git clone https://github.com/VikasOjha666/Data-generator-for-CRNN.git\n",
    "%cd Data-generator-for-CRNN\n",
    "!mkdir images/\n",
    "!python3 generate_data.py --n_samples 300000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UwcIFwjg4IN-",
    "outputId": "38b2035b-40ad-46b6-a809-e04096efc6b0"
   },
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.activations import relu, sigmoid, softmax\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "oNmZv6iY4Loh"
   },
   "outputs": [],
   "source": [
    "#char_list = string.ascii_letters+string.digits\n",
    "\n",
    "punclist='.?:;\"'\n",
    "punclist2=\"-+/()[]!`,|*&^%$#@'\"\n",
    "\n",
    "#Character sets to choose from.\n",
    "smallletters=string.ascii_lowercase\n",
    "capitalletters=string.ascii_uppercase\n",
    "digits=string.digits\n",
    "char_list=smallletters#+capitalletters+digits+punclist+punclist2\n",
    "\n",
    "chars = defaultdict(int)\n",
    " \n",
    "def encode_to_labels(txt):\n",
    "    # encoding each output word into digits\n",
    "    dig_lst = []\n",
    "    for index, char in enumerate(txt):\n",
    "        try:\n",
    "            dig_lst.append(char_list.index(char))\n",
    "        except:\n",
    "            print(char)\n",
    "        \n",
    "    return dig_lst\n",
    "\n",
    "def find_dominant_color(image):\n",
    "        #Resizing parameters\n",
    "        width, height = 150,150\n",
    "        image = image.resize((width, height),resample = 0)\n",
    "        #Get colors from image object\n",
    "        pixels = image.getcolors(width * height)\n",
    "        #Sort them by count number(first element of tuple)\n",
    "        sorted_pixels = sorted(pixels, key=lambda t: t[0])\n",
    "        #Get the most frequent color\n",
    "        dominant_color = sorted_pixels[-1][1]\n",
    "        return dominant_color\n",
    "\n",
    "def preprocess_img(img, imgSize):\n",
    "    \"put img into target img of size imgSize, transpose for TF and normalize gray-values\"\n",
    "\n",
    "    # there are damaged files in IAM dataset - just use black image instead\n",
    "    if img is None:\n",
    "        img = np.zeros([imgSize[1], imgSize[0]]) \n",
    "        print(\"Image None!\")\n",
    "\n",
    "    # create target image and copy sample image into it\n",
    "    (wt, ht) = imgSize\n",
    "    (h, w) = img.shape\n",
    "    fx = w / wt\n",
    "    fy = h / ht\n",
    "    f = max(fx, fy)\n",
    "    newSize = (max(min(wt, int(w / f)), 1),\n",
    "               max(min(ht, int(h / f)), 1))  # scale according to f (result at least 1 and at most wt or ht)\n",
    "    img = cv2.resize(img, newSize, interpolation=cv2.INTER_CUBIC) # INTER_CUBIC interpolation best approximate the pixels image\n",
    "                                                               # see this https://stackoverflow.com/a/57503843/7338066\n",
    "    most_freq_pixel=find_dominant_color(Image.fromarray(img))\n",
    "    target = np.ones([ht, wt]) * most_freq_pixel  \n",
    "    target[0:newSize[1], 0:newSize[0]] = img\n",
    "\n",
    "    img = target\n",
    "\n",
    "    return img\n",
    "\n",
    "def counting_characters(labels):\n",
    "    for label in labels:\n",
    "        for char in label:\n",
    "            chars[char] += 1\n",
    "    return chars\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyz\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "print(char_list)\n",
    "print(len(char_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_img = []\n",
    "training_txt = []\n",
    "train_input_length = []\n",
    "train_label_length = []\n",
    "orig_txt = []\n",
    " \n",
    "#lists for validation dataset\n",
    "valid_img = []\n",
    "valid_txt = []\n",
    "valid_input_length = []\n",
    "valid_label_length = []\n",
    "valid_orig_txt = []\n",
    " \n",
    "max_label_len = 0\n",
    "\n",
    "annot=open('content/Data-generator-for-CRNN/annotation.txt','r').readlines()\n",
    "imagenames=[]\n",
    "txts=[]\n",
    "\n",
    "for cnt in annot:\n",
    "\n",
    "    filename,txt=cnt.split('~')[0],cnt.split('~')[1].split('\\n')[0]\n",
    "    imagenames.append(filename)# nazwa pliku\n",
    "    txts.append(txt)# to jaki text zawiera\n",
    "    \n",
    "    #print(cnt.split('~')[0])\n",
    "    #print(cnt.split('~')[1].split('\\n')[0])\n",
    "    #print()\n",
    "\n",
    "chars=counting_characters(txts)\n",
    "\n",
    "c = list(zip(imagenames, txts)) # połączenie nazwy z textem\n",
    "\n",
    "#random.shuffle(c) #wymieszanie kolejności //chociaż jak są generowane to i tak chyba bez sensu\n",
    "\n",
    "imagenames, txts = zip(*c)\n",
    "    \n",
    "\n",
    "    \n",
    "for i in range(len(imagenames)):# dla każdego obrazka\n",
    "        img = cv2.imread('content/Data-generator-for-CRNN/images/'+imagenames[i],0)   \n",
    " \n",
    "        img=preprocess_img(img,(128,32)) #najpierw preproces obrazu\n",
    "        img=np.expand_dims(img,axis=-1)\n",
    "        img = img/255.\n",
    "        txt = txts[i]\n",
    "        \n",
    "        # compute maximum length of the text\n",
    "        if len(txt) > max_label_len:\n",
    "            max_label_len = len(txt)\n",
    "            \n",
    "        #print(img)   \n",
    "        # split the 150000 data into validation and training dataset as 10% and 90% respectively\n",
    "        if i%10 == 0:     \n",
    "            valid_orig_txt.append(txt)   \n",
    "            valid_label_length.append(len(txt))\n",
    "            valid_input_length.append(31)\n",
    "            valid_img.append(img)\n",
    "            valid_txt.append(encode_to_labels(txt))\n",
    "        else:\n",
    "            orig_txt.append(txt)   \n",
    "            train_label_length.append(len(txt))\n",
    "            train_input_length.append(31)\n",
    "            training_img.append(img)\n",
    "            training_txt.append(encode_to_labels(txt)) \n",
    "        \n",
    "        # break the loop if total data is 200000\n",
    "        if i == 200000:\n",
    "            flag = 1\n",
    "            break\n",
    "        i+=1\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wcale sie nie zacialem:  0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "annot=open('content/Data-generator-for-CRNN/annotation.txt','r').readlines()\n",
    "imagenames=[]\n",
    "txts=[]\n",
    "\n",
    "for cnt in annot:\n",
    "\n",
    "    filename,txt=cnt.split('~')[0],cnt.split('~')[1].split('\\n')[0]\n",
    "    imagenames.append(filename)\n",
    "    txts.append(txt)\n",
    "c = list(zip(imagenames, txts))\n",
    "\n",
    "random.shuffle(c)\n",
    "\n",
    "imagenames, txts = zip(*c)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    i=0\n",
    "    def __init__(self, dataset,words,max_label_len,char_list, batch_size=1,dim=(1), shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset = dataset\n",
    "        self.words = words\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = 0\n",
    "        self.max_label_len=max_label_len\n",
    "        self.char_list=char_list\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch' \n",
    "        return int(np.floor(len(self.dataset) / self.batch_size))\n",
    "        #return math.ceil(len(self.dataset) / self.batch_size)\n",
    "        \n",
    "    def __getitem__(self, index):# chyba indexuje dla batcha ale nwm\n",
    "        'Generate one batch of data'\n",
    "        training_img = []\n",
    "        training_txt = []\n",
    "        train_input_length = []\n",
    "        train_label_length = []\n",
    "        orig_txt = []\n",
    "        \n",
    "        \n",
    "        img = cv2.imread('content/Data-generator-for-CRNN/images/'+imagenames[self.i],0)\n",
    "        img=preprocess_img(img,(128,32))\n",
    "        img=np.expand_dims(img,axis=-1)\n",
    "        img = img/255.\n",
    "        txt = txts[self.i]\n",
    "\n",
    "        orig_txt.append(txt)   \n",
    "        train_label_length.append(len(txt))\n",
    "        train_input_length.append(31)\n",
    "        training_img.append(img)\n",
    "        training_txt.append(encode_to_labels(txt)) \n",
    "        print(\"getitem index: \",index)\n",
    "            #jeśli nie nadpisuje to trzeba będzie zerować z np.array()\n",
    "        train_padded_txt = pad_sequences(training_txt, maxlen=self.max_label_len, padding='post', value = len(self.char_list))\n",
    "        valid_padded_txt = pad_sequences(valid_txt, maxlen=self.max_label_len, padding='post', value = len(self.char_list))\n",
    "        training_img = np.array(training_img)\n",
    "        train_input_length = np.array(train_input_length)\n",
    "        train_label_length = np.array(train_label_length)\n",
    "\n",
    "       # valid_img = np.array(valid_img)\n",
    "       # valid_input_length = np.array(valid_input_length)\n",
    "       # valid_label_length = np.array(valid_label_length)\n",
    "        self.i=self.i+1\n",
    "        #rint([training_img, train_padded_txt, train_input_length, train_label_length],np.zeros(len(training_img)))\n",
    "        return [training_img, train_padded_txt, train_input_length, train_label_length],np.zeros(len(training_img))\n",
    "         #x=[training_img, train_padded_txt, train_input_length, train_label_length]\n",
    "         #y=np.zeros(len(training_img)),\n",
    "    def on_epoch_end(self):\n",
    "        print(\"Wcale sie nie zacialem: \",self.i)\n",
    "   #    'Updates indexes after each epoch'\n",
    "   #    self.indexes = np.arange(len(dataset))\n",
    "    #   if self.shuffle == True:\n",
    "     #      np.random.shuffle(self.indexes)\n",
    "                     #max label len do dodania zapisane do pliku przy generowaniu i tu odczytane z pliku bedzie\n",
    "train_generator = DataGenerator(dataset=imagenames,words=txts,max_label_len=max_label_len,char_list=char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(imagenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(max_label_len)# do odczytania przy generowaniu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a :  40090\n",
      "b :  40194\n",
      "c :  40141\n",
      "d :  40387\n",
      "e :  40838\n",
      "f :  40533\n",
      "g :  40553\n",
      "h :  40183\n",
      "i :  40330\n",
      "j :  39957\n",
      "k :  40220\n",
      "l :  40144\n",
      "m :  40542\n",
      "n :  40307\n",
      "o :  40240\n",
      "p :  40485\n",
      "q :  40437\n",
      "r :  40475\n",
      "s :  40309\n",
      "t :  40143\n",
      "u :  40091\n",
      "v :  40143\n",
      "w :  39884\n",
      "x :  40475\n",
      "y :  40565\n",
      "z :  40090\n"
     ]
    }
   ],
   "source": [
    "for x in char_list:\n",
    "    print(x,\": \",chars[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kp-U2u0p4Y_T"
   },
   "outputs": [],
   "source": [
    "#pad each output label to maximum text length\n",
    " \n",
    "train_padded_txt = pad_sequences(training_txt, maxlen=max_label_len, padding='post', value = len(char_list))\n",
    "valid_padded_txt = pad_sequences(valid_txt, maxlen=max_label_len, padding='post', value = len(char_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SqWcDplF6HAt",
    "outputId": "449c3359-389e-445b-d7cc-7bc682fd2328"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 09:24:19.112019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 09:24:19.142577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 09:24:19.142784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 09:24:19.143409: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-30 09:24:19.144690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 09:24:19.144867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 09:24:19.145026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 09:24:19.587587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 09:24:19.587783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 09:24:19.587913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 09:24:19.588042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:0a:00.0, compute capability: 8.6\n",
      "2022-05-30 09:24:29.617618: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.50MiB (rounded to 4718592)requested by op RandomUniform\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2022-05-30 09:24:29.617659: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc\n",
      "2022-05-30 09:24:29.617670: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256): \tTotal Chunks: 12, Chunks in use: 12. 3.0KiB allocated for chunks. 3.0KiB in use in bin. 300B client-requested in use in bin.\n",
      "2022-05-30 09:24:29.617677: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512): \tTotal Chunks: 1, Chunks in use: 1. 512B allocated for chunks. 512B in use in bin. 512B client-requested in use in bin.\n",
      "2022-05-30 09:24:29.617684: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024): \tTotal Chunks: 3, Chunks in use: 3. 3.5KiB allocated for chunks. 3.5KiB in use in bin. 3.0KiB client-requested in use in bin.\n",
      "2022-05-30 09:24:29.617691: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048): \tTotal Chunks: 1, Chunks in use: 1. 2.2KiB allocated for chunks. 2.2KiB in use in bin. 2.2KiB client-requested in use in bin.\n",
      "2022-05-30 09:24:29.617698: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-05-30 09:24:29.617703: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-05-30 09:24:29.617709: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-05-30 09:24:29.617714: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-05-30 09:24:29.617720: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-05-30 09:24:29.617725: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-05-30 09:24:29.617734: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144): \tTotal Chunks: 1, Chunks in use: 1. 288.0KiB allocated for chunks. 288.0KiB in use in bin. 288.0KiB client-requested in use in bin.\n",
      "2022-05-30 09:24:29.617740: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288): \tTotal Chunks: 1, Chunks in use: 0. 575.5KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-05-30 09:24:29.617747: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576): \tTotal Chunks: 1, Chunks in use: 1. 1.12MiB allocated for chunks. 1.12MiB in use in bin. 1.12MiB client-requested in use in bin.\n",
      "2022-05-30 09:24:29.617753: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152): \tTotal Chunks: 2, Chunks in use: 0. 4.50MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-05-30 09:24:29.617760: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304): \tTotal Chunks: 1, Chunks in use: 1. 4.15MiB allocated for chunks. 4.15MiB in use in bin. 2.25MiB client-requested in use in bin.\n",
      "2022-05-30 09:24:29.617765: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-05-30 09:24:29.617771: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-05-30 09:24:29.617776: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-05-30 09:24:29.617782: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-05-30 09:24:29.617790: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-05-30 09:24:29.617795: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-05-30 09:24:29.617802: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 4.50MiB was 4.00MiB, Chunk State: \n",
      "2022-05-30 09:24:29.617807: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 11141120\n",
      "2022-05-30 09:24:29.617818: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f7e3f000000 of size 1280 next 1\n",
      "2022-05-30 09:24:29.617825: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f7e3f000500 of size 256 next 2\n",
      "2022-05-30 09:24:29.617832: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f7e3f000600 of size 256 next 3\n",
      "2022-05-30 09:24:29.617836: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f7e3f000700 of size 256 next 4\n",
      "2022-05-30 09:24:29.617841: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f7e3f000800 of size 256 next 5\n",
      "2022-05-30 09:24:29.617846: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f7e3f000900 of size 256 next 8\n",
      "2022-05-30 09:24:29.617850: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f7e3f000a00 of size 256 next 9\n",
      "2022-05-30 09:24:29.617855: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f7e3f000b00 of size 512 next 10\n",
      "2022-05-30 09:24:29.617860: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f7e3f000d00 of size 256 next 13\n",
      "2022-05-30 09:24:29.617864: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f7e3f000e00 of size 256 next 14\n",
      "2022-05-30 09:24:29.617869: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f7e3f000f00 of size 1024 next 15\n",
      "2022-05-30 09:24:29.617874: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f7e3f001300 of size 256 next 18\n",
      "2022-05-30 09:24:29.617878: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f7e3f001400 of size 256 next 19\n",
      "2022-05-30 09:24:29.617883: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f7e3f001500 of size 1280 next 6\n",
      "2022-05-30 09:24:29.617888: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f7e3f001a00 of size 2304 next 7\n",
      "2022-05-30 09:24:29.617892: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f7e3f002300 of size 256 next 20\n",
      "2022-05-30 09:24:29.617897: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f7e3f002400 of size 256 next 22\n",
      "2022-05-30 09:24:29.617902: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f7e3f002500 of size 589312 next 12\n",
      "2022-05-30 09:24:29.617906: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f7e3f092300 of size 294912 next 11\n",
      "2022-05-30 09:24:29.617911: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f7e3f0da300 of size 2359296 next 17\n",
      "2022-05-30 09:24:29.617916: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f7e3f31a300 of size 1179648 next 16\n",
      "2022-05-30 09:24:29.617920: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f7e3f43a300 of size 2359296 next 21\n",
      "2022-05-30 09:24:29.617926: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f7e3f67a300 of size 4349184 next 18446744073709551615\n",
      "2022-05-30 09:24:29.617930: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: \n",
      "2022-05-30 09:24:29.617936: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 12 Chunks of size 256 totalling 3.0KiB\n",
      "2022-05-30 09:24:29.617942: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 512 totalling 512B\n",
      "2022-05-30 09:24:29.617949: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1024 totalling 1.0KiB\n",
      "2022-05-30 09:24:29.617955: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 1280 totalling 2.5KiB\n",
      "2022-05-30 09:24:29.617962: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 2304 totalling 2.2KiB\n",
      "2022-05-30 09:24:29.617970: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 294912 totalling 288.0KiB\n",
      "2022-05-30 09:24:29.617976: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1179648 totalling 1.12MiB\n",
      "2022-05-30 09:24:29.617981: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4349184 totalling 4.15MiB\n",
      "2022-05-30 09:24:29.617986: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 5.56MiB\n",
      "2022-05-30 09:24:29.617991: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 11141120 memory_limit_: 11141120 available bytes: 0 curr_region_allocation_bytes_: 22282240\n",
      "2022-05-30 09:24:29.618001: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: \n",
      "Limit:                        11141120\n",
      "InUse:                         5833216\n",
      "MaxInUse:                     10550016\n",
      "NumAllocs:                          32\n",
      "MaxAllocSize:                  4349184\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2022-05-30 09:24:29.618008: W tensorflow/core/common_runtime/bfc_allocator.cc:474] *____****____________________***********____________________***********************xxxxxxxxxxxxxxxxx\n",
      "2022-05-30 09:24:29.618076: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at random_op.cc:74 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[3,3,256,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[3,3,256,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RandomUniform]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# poolig layer with kernel size (2,1)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m pool_4 \u001b[38;5;241m=\u001b[39m MaxPool2D(pool_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))(conv_4)\n\u001b[0;32m---> 17\u001b[0m conv_5 \u001b[38;5;241m=\u001b[39m \u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msame\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool_4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Batch normalization layer\u001b[39;00m\n\u001b[1;32m     19\u001b[0m batch_norm_5 \u001b[38;5;241m=\u001b[39m BatchNormalization()(conv_5)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/backend.py:1920\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[0;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generator:\n\u001b[1;32m   1918\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generator\u001b[38;5;241m.\u001b[39muniform(\n\u001b[1;32m   1919\u001b[0m       shape\u001b[38;5;241m=\u001b[39mshape, minval\u001b[38;5;241m=\u001b[39mminval, maxval\u001b[38;5;241m=\u001b[39mmaxval, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m-> 1920\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1921\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_legacy_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[3,3,256,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RandomUniform]"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(32,128,1))\n",
    " \n",
    "# convolution layer with kernel size (3,3)\n",
    "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
    "# poolig layer with kernel size (2,2)\n",
    "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
    " \n",
    "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
    "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
    " \n",
    "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
    " \n",
    "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
    "# poolig layer with kernel size (2,1)\n",
    "pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
    " \n",
    "conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
    "# Batch normalization layer\n",
    "batch_norm_5 = BatchNormalization()(conv_5)\n",
    " \n",
    "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
    "batch_norm_6 = BatchNormalization()(conv_6)\n",
    "pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
    "\n",
    " \n",
    "conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
    " \n",
    "squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
    "\n",
    "# bidirectional LSTM layers with units=128\n",
    "blstm_1 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(squeezed)\n",
    "blstm_2 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(blstm_1)\n",
    " \n",
    "outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
    "\n",
    "# model to be used at test time\n",
    "act_model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBtmnGNn6I0J",
    "outputId": "cf4688c0-0540-47fa-e14c-d401d81ac38b"
   },
   "outputs": [],
   "source": [
    "act_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TMxpZweM6MfI",
    "outputId": "dfe1580c-ef52-482b-dcf6-2a392523b4e3"
   },
   "outputs": [],
   "source": [
    "labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
    "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    " \n",
    " \n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    " \n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    " \n",
    " \n",
    "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])\n",
    "\n",
    "#model to be used at training time\n",
    "model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhqs_1FE6NYR"
   },
   "outputs": [],
   "source": [
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam',metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=35) \n",
    "    \n",
    "filepath='male.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "callbacks_list = [checkpoint,es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2LhU9Lpj6QU-"
   },
   "outputs": [],
   "source": [
    "training_img = np.array(training_img)\n",
    "train_input_length = np.array(train_input_length)\n",
    "train_label_length = np.array(train_label_length)\n",
    "\n",
    "valid_img = np.array(valid_img)\n",
    "valid_input_length = np.array(valid_input_length)\n",
    "valid_label_length = np.array(valid_label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bx41yVYP9tQh",
    "outputId": "91a5bfeb-a85d-4ccf-b948-90320e8af220",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#batch_size = 256\n",
    "epochs = 5\n",
    "history = model.fit(train_generator, \n",
    "                    epochs = epochs,validation_data = ([valid_img, valid_padded_txt, \n",
    "                    valid_input_length, valid_label_length],[np.zeros(len(valid_img))]),\n",
    "                    verbose = 1, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pW4mPMB3MpCF"
   },
   "outputs": [],
   "source": [
    "# load the saved best model weights\n",
    "act_model.load_weights('male.hdf5')\n",
    " \n",
    "# predict outputs on validation images\n",
    "prediction = act_model.predict(valid_img[10:20])\n",
    " \n",
    "# use CTC decoder\n",
    "out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
    "                         greedy=True)[0][0])\n",
    " \n",
    "# see the results\n",
    "i = 10\n",
    "for x in out:\n",
    "    print(\"original_text = \", valid_orig_txt[i])\n",
    "    print(\"predicted text = \", end = '')\n",
    "    for p in x:  \n",
    "        if int(p) != -1:\n",
    "            print(char_list[int(p)], end = '')       \n",
    "    print('\\n')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train','Validation'], loc='upper left')\n",
    "plt.savefig('my_plot.png')\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CRNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
